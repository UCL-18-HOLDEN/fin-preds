{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File merged_standardized.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-13426f321501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'merged_standardized.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelholden/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelholden/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelholden/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelholden/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelholden/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File merged_standardized.csv does not exist"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import pandas as pd\n",
    "import theano\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "data = pd.read_csv('merged_standardized.csv')\n",
    "\n",
    "\n",
    "\n",
    "company_list=data['conm'].unique()\n",
    "company_list\n",
    "print company_list\n",
    "print 'count is: '+str(len(company_list))\n",
    "data = data[(data['conm']==company_list[1])|(data['conm']==company_list[2])|(data['conm']==company_list[3])\n",
    "            |(data['conm']==company_list[4])|(data['conm']==company_list[5])|(data['conm']==company_list[6])\n",
    "            |(data['conm']==company_list[7])|(data['conm']==company_list[8])|(data['conm']==company_list[9])\n",
    "            |(data['conm']==company_list[10])|(data['conm']==company_list[11])\n",
    "            |(data['conm']==company_list[12])|(data['conm']==company_list[13])|(data['conm']==company_list[14])\n",
    "            |(data['conm']==company_list[15])|(data['conm']==company_list[16])|(data['conm']==company_list[17])\n",
    "            |(data['conm']==company_list[18])|(data['conm']==company_list[19])]\n",
    "\n",
    "data = data[(data['conm']==company_list[1])|(data['conm']==company_list[3])|(data['conm']==company_list[4])\n",
    "            |(data['conm']==company_list[5])|(data['conm']==company_list[10])|(data['conm']==company_list[15])\n",
    "            |(data['conm']==company_list[19])]\n",
    "\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True,inplace=True)\n",
    "data['sunday_beginning'] = pd.to_datetime(data['sunday_beginning'],format='%Y/%m/%d')\n",
    "data.sort_values(by='sunday_beginning',inplace=True)\n",
    "\n",
    "data['conm_code'] = le.fit_transform(data.conm)\n",
    "\n",
    "data['weekly_direction'] = data['weekly_pct']>0\n",
    "data['weekly_direction'] = data['weekly_direction'].astype(int)\n",
    "\n",
    "data['weekly_direction'] = data['weekly_direction'].astype(theano.config.floatX)\n",
    "data['weekly_pct'] = data['weekly_pct'].astype(theano.config.floatX)\n",
    "\n",
    "split = np.floor(0.7*data.shape[0]).astype(int)\n",
    "\n",
    "data_train = data.iloc[:split,:]\n",
    "data_test = data.iloc[split:,:]\n",
    "\n",
    "conm_idx = data_train.conm_code.values\n",
    "\n",
    "\n",
    "n_companies = len(data.conm.unique())\n",
    "\n",
    "data.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['conm']==company_list[0])|(data['conm']==company_list[1])|(data['conm']==company_list[2])|(data['conm']==company_list[3])|(data['conm']==company_list[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print data.shape\n",
    "data[['conm', 'weekly_pct', 'orders_one']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pm.Model() as unpooled_model:\n",
    "#\n",
    "#    # Independent parameters for each county\n",
    "#    a = pm.Normal('a', 0, sd=100, shape=n_counties)\n",
    "#    b = pm.Normal('b', 0, sd=100, shape=n_counties)\n",
    "#\n",
    "#    # Model error\n",
    "#    eps = pm.HalfCauchy('eps', 5)\n",
    "#\n",
    "#    # Model prediction of radon level\n",
    "#    # a[county_idx] translates to a[0, 0, 0, 1, 1, ...],\n",
    "#    # we thus link multiple household measures of a county\n",
    "#    # to its coefficients.\n",
    "#    radon_est = a[county_idx] + b[county_idx]*data.floor.values\n",
    "#\n",
    "#    # Data likelihood\n",
    "#    y = pm.Normal('y', radon_est, sd=eps, observed=data.log_radon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with unpooled_model:\n",
    "#    unpooled_trace = pm.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hierarchical_model:\n",
    "    # Hyperpriors for group nodes\n",
    "    mu_a = pm.Normal('mu_a', mu=0., sd=100**2)\n",
    "    sigma_a = pm.HalfCauchy('sigma_a', 5)\n",
    "    mu_b = pm.Normal('mu_b', mu=0., sd=100**2)\n",
    "    sigma_b = pm.HalfCauchy('sigma_b', 5)\n",
    "    mu_b1 = pm.Normal('mu_b1', mu=0., sd=100**2)\n",
    "    sigma_b1 = pm.HalfCauchy('sigma_b1', 5)\n",
    "    mu_b2 = pm.Normal('mu_b2', mu=0., sd=100**2)\n",
    "    sigma_b2 = pm.HalfCauchy('sigma_b2', 5)\n",
    "    mu_b3 = pm.Normal('mu_b3', mu=0., sd=100**2)\n",
    "    sigma_b3 = pm.HalfCauchy('sigma_b3', 5)\n",
    "    mu_b4 = pm.Normal('mu_b4', mu=0., sd=100**2)\n",
    "    sigma_b4 = pm.HalfCauchy('sigma_b4', 5)\n",
    "    mu_b5 = pm.Normal('mu_b5', mu=0., sd=100**2)\n",
    "    sigma_b5 = pm.HalfCauchy('sigma_b5', 5)\n",
    "\n",
    "    # Intercept for each conm, distributed around group mean mu_a\n",
    "    # Above we just set mu and sd to a fixed value while here we\n",
    "    # plug in a common group distribution for all a and b (which are\n",
    "    # vectors of length n_companies).\n",
    "    a = pm.Normal('a', mu=mu_a, sd=sigma_a, shape=n_companies)\n",
    "    # beta 1 through 6\n",
    "    b = pm.Normal('b', mu=mu_b, sd=sigma_b, shape=n_companies)\n",
    "    b1 = pm.Normal('b1', mu=mu_b1, sd=sigma_b1, shape=n_companies)\n",
    "    b2 = pm.Normal('b2', mu=mu_b2, sd=sigma_b2, shape=n_companies)\n",
    "    b3 = pm.Normal('b3', mu=mu_b3, sd=sigma_b3, shape=n_companies)\n",
    "    b4 = pm.Normal('b4', mu=mu_b4, sd=sigma_b4, shape=n_companies)\n",
    "    b5 = pm.Normal('b5', mu=mu_b5, sd=sigma_b5, shape=n_companies)\n",
    "\n",
    "    # Model error\n",
    "    sigma = pm.Gamma(\"sigma\", alpha=10, beta=1)\n",
    "    \n",
    "    \n",
    "\n",
    "    ret_est = a[conm_idx] + b[conm_idx] * data_train.orders_one.values + b1[conm_idx] * data_train.orders_two.values + b2[conm_idx] * data_train.orders_three.values + b3[conm_idx] * data_train.orders_four.values + b4[conm_idx] * data_train.orders_five.values + b5[conm_idx] * data_train.orders_six.values\n",
    "\n",
    "    # Data likelihood\n",
    "    ret_like = pm.Normal('dir_like', mu=ret_est, sd=sigma, observed=data_train.weekly_pct)\n",
    "    #dir_like = pm.Logistic('dir_like', mu=dir_est, s=sigma, observed=data_train.weekly_direction)\n",
    "    #dir_like = pm.Bernoulli('dir_like',p=dir_est, observed=data_train.weekly_pct)\n",
    "    \n",
    "    \n",
    "    #COMMENT THIS OUT TO GET DEFAULT MC SAMPLER\n",
    "    #step = pm.step_methods.hmc.NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference button (TM)!\n",
    "with hierarchical_model:\n",
    "    hierarchical_trace = pm.sample(draws=5000,init='adapt_diag', n_init=1000, progressbar=True,)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(hierarchical_trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hier_a = hierarchical_trace['b1'][2000:3000,3]\n",
    "hier_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    hier_a = hierarchical_trace['b4'][4500:,i]\n",
    "    print hier_a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {}\n",
    "\n",
    "for i in range(7):\n",
    "    param_list=[]\n",
    "    intercept = np.mean(hierarchical_trace['a'][4500:,i])\n",
    "    beta_1 = np.mean(hierarchical_trace['b'][4500:,i])\n",
    "    beta_2 = np.mean(hierarchical_trace['b1'][4500:,i])\n",
    "    beta_3 = np.mean(hierarchical_trace['b2'][4500:,i] )\n",
    "    beta_4 = np.mean(hierarchical_trace['b3'][4500:,i])\n",
    "    beta_5 = np.mean(hierarchical_trace['b4'][4500:,i] )\n",
    "    beta_6 = np.mean(hierarchical_trace['b5'][4500:,i])\n",
    "    \n",
    "    param_list.append(intercept)\n",
    "    param_list.append(beta_1)\n",
    "    param_list.append(beta_2)\n",
    "    param_list.append(beta_3)\n",
    "    param_list.append(beta_4)\n",
    "    param_list.append(beta_5)\n",
    "    param_list.append(beta_6)\n",
    "    \n",
    "    param_dict[i] = param_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict[0][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we make check the index of the conm. And finally we matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conm_to_code = data.drop_duplicates(subset=['conm_code'])\n",
    "conm_to_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data['prediction']=0\n",
    "#\n",
    "#code=4\n",
    "#\n",
    "#data_0 = data_test[data_test['conm_code']==code]\n",
    "#data_0['prediction'] = 1.0/(1.0+np.exp(-(param_dict[code][0]+param_dict[code][1]*data_0['ratio_one']+param_dict[code][2]*data_0['ratio_two']+param_dict[code][0]*data_0['ratio_three']+param_dict[code][0]*data_0['ratio_four']+param_dict[code][0]*data_0['ratio_five']+param_dict[code][0]*data_0['ratio_six'])))\n",
    "#data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    \n",
    "    data_test['prediction']=0\n",
    "    \n",
    "    code=i\n",
    "    \n",
    "    data_0 = data_test[data_test['conm_code']==code]\n",
    "    data_0['prediction'] = param_dict[code][0]+param_dict[code][1]*data_0['orders_one']+param_dict[code][2]*data_0['orders_two']+param_dict[code][3]*data_0['orders_three']+param_dict[code][4]*data_0['orders_four']+param_dict[code][5]*data_0['orders_five']+param_dict[code][6]*data_0['orders_six']\n",
    "    data_0.sort_values(by='prediction',inplace=True,ascending=False)\n",
    "    \n",
    "    for k in range(7):\n",
    "        if code==k:\n",
    "            data_0.to_csv('company_'+str(k)+'_preds_reg.csv',index=False)\n",
    "    \n",
    "    #if code==0:\n",
    "    #    data_0.to_csv('amazon_preds_bern_log.csv',index=False)\n",
    "    #if code==1:\n",
    "    #    data_0.to_csv('bbby_preds_bern_log.csv',index=False)\n",
    "    #if code==2:\n",
    "    #    data_0.to_csv('bestbuy_preds_bern_log.csv',index=False)\n",
    "    #if code==3:\n",
    "    #    data_0.to_csv('ebay_preds_bern_log.csv',index=False)\n",
    "    #if code==4:\n",
    "    #    data_0.to_csv('etsy_preds_bern_log.csv',index=False)\n",
    "    ##if code==5:\n",
    "    ##    data_0.to_csv('expedia_preds_bern_log.csv',index=False)\n",
    "    #if code==6:\n",
    "    #    data_0.to_csv('gamestop_preds_bern_log.csv',index=False)\n",
    "    #if code==7:\n",
    "    #    data_0.to_csv('gap_preds_bern_log.csv',index=False)\n",
    "    #if code==8:\n",
    "    #    data_0.to_csv('homedepot_preds_bern_log.csv',index=False)\n",
    "    #if code==9:\n",
    "    #    data_0.to_csv('kirkland_preds_bern_log.csv',index=False)\n",
    "    #if code==10:\n",
    "    #    data_0.to_csv('lowes_preds_bern_log.csv',index=False)\n",
    "    #if code==11:\n",
    "    #    data_0.to_csv('nike_preds_bern_log.csv',index=False)\n",
    "    #if code==12:\n",
    "    #    data_0.to_csv('nordstrom_preds_bern_log.csv',index=False)\n",
    "        \n",
    "a = pd.read_csv('company_1_preds_reg.csv')\n",
    "b = pd.read_csv('company_2_preds_reg.csv')\n",
    "c = pd.read_csv('company_3_preds_reg.csv')\n",
    "d = pd.read_csv('company_4_preds_reg.csv')\n",
    "e = pd.read_csv('company_5_preds_reg.csv')\n",
    "f = pd.read_csv('company_6_preds_reg.csv')\n",
    "#g = pd.read_csv('company_7_preds_bern_log.csv')\n",
    "#h = pd.read_csv('company_8_preds_bern_log.csv')\n",
    "#i = pd.read_csv('company_9_preds_bern_log.csv')\n",
    "j = pd.read_csv('company_0_preds_reg.csv')\n",
    "#k = pd.read_csv('company_10_preds_bern_log.csv')\n",
    "#l = pd.read_csv('company_11_preds_bern_log.csv')\n",
    "#m = pd.read_csv('company_12_preds_bern_log.csv')\n",
    "#n = pd.read_csv('company_13_preds_bern_log.csv')\n",
    "#o = pd.read_csv('company_14_preds_bern_log.csv')\n",
    "#p = pd.read_csv('company_15_preds_bern_log.csv')\n",
    "#q = pd.read_csv('company_16_preds_bern_log.csv')\n",
    "#r = pd.read_csv('company_17_preds_bern_log.csv')\n",
    "#s = pd.read_csv('company_18_preds_bern_log.csv')\n",
    "\n",
    "merg = pd.concat([a,b,c,d,j,e,f])\n",
    "merg.to_csv('full_predictions_reg.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
